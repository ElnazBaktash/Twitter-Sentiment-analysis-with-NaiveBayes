{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                              In the name of God\n",
    "                                       \n",
    "To do this project,the article i have choosen is \"Twitter Sentiment Classification Using Naive Bayes Based On Trainer Perception\".\n",
    "This paper presents strategy to classify tweets sentiment using Naïve Bayes techniques based on trainers’perception into three categories; positive, negative or neutral. In this study, there were 27 trainers participated.Each trainer was asked to classify the sentiment of tweets of each keyword.Results from the classification training was then be used as the input for Naïve Bayes training for the remaining 25 tweets.\n",
    "according to the subject, the first part of the article which relates to the trainners, it says like this the input of the naive bayes algorithm is supervised. so for this reason i have choosen train's data with it's target.\n",
    "the steps in this article are as follows:\n",
    "\n",
    "1. selecting tweets(train's data)\n",
    "2. PreProcessing \n",
    "3. Classify tweet sentiment by program\n",
    "4. validate the classified sentiment by test's data\n",
    "\n",
    "In this article the automatic classification is being done by utilizing Python NLTK Library.\n",
    "\n",
    "nltk.classify.naivebayes: A classifier based on the Naive Bayes algorithm.  In order to find the probability for a label, this algorithm first uses the Bayes rule to express P(label|features) in terms of P(label) and P(features|label):\n",
    "     \n",
    "             P(label|features)= ( P(label) * P(features|label) ) / P(features)\n",
    "             \n",
    "The algorithm then makes the 'naive' assumption that all features are independent, given the label:\n",
    "                       \n",
    "                 P(label|features)= ( P(label) * P(f1|label) * ... * P(fn|label) ) / P(features)\n",
    "                 \n",
    "Rather than computing P(features) explicitly, the algorithm just calculates the numerator for each label, and normalizes them so they sum to one:\n",
    "\n",
    "                  \n",
    "           P(label|features)= ( P(label) * P(f1|label) * ... * P(fn|label) ) / SUM[l]( P(l) * P(f1|l) * ... * P(fn|l) )\n",
    "\n",
    "- P(label) gives the probability that an input will receive each label, given no information about the input's features.\n",
    "- P(fname=fval|label) gives the probability that a given feature (fname) will receive a given value (fval), given that the\n",
    "  label (label).\n",
    "\n",
    "If the classifier encounters an input with a feature that has never been seen with any label, then rather than assigning a probability of 0 to all labels, it will ignore that feature.\n",
    "\n",
    "The feature value 'None' is reserved for unseen feature values; you generally should not use 'None' as a feature value for one of your own features.\n",
    "\n",
    "- Getting Data(selecting tweets(train's data)):\n",
    "\n",
    "one way to get data use the twitter's api.To use twitter’s API, we need to first create a twitter account. Afterwards, we go to apps.twitter.com and create an app.After that go to “Keys and Access tokens” and get your API key and secret (copy and save them for later).Then go and create your access token (you will find this by scrolling down), and then save your access token and access token secret.These codes will allow us to access twitter’s API through python. Its pretty much the key needed to access twitter’s database. \n",
    "\n",
    "Unfortunately we can not access that address because we live in Iran!!!!!!!\n",
    "so i downloaded the sample of twitter's data with it's lables as train's data from internet.\n",
    "\n",
    "- PreProcessing :\n",
    "\n",
    "The preprocessing stages is where all the tweets get clean up. The process steps are: \n",
    "\n",
    "- set all tweets to lowerCase\n",
    "- convert www.* or https:// to Url\n",
    "- convert @userName to AT_User\n",
    "- HashTagSplit(replace #Hashtage with Hashtag)\n",
    "- Trim\n",
    "\n",
    "\n",
    " - Classify tweet sentiment by program:\n",
    "This section is specified in the code with comment \"NaiveBayes Classifier\", in this section i defined a method that's givent 2 input argument (train's data & test's data) and first in train's data do some process such a preprocessing and feature extraction and finally with nltk.NaiveBayesClassifier, classify train's data then it uses the result to classify test's data, which is the 4th step(validate the classified sentiment by test's data).\n",
    "\n",
    "In the file I sent you,there is a sample of train's data & test's data and an example of classified data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "@PrincessSuperC Hey Cici sweetheart! Just wanted to let u know I luv u! OH! and will the mixtape drop soon? FANTASY RIDE MAY 5TH!!!!\n",
      "positive\n",
      "\n",
      "\n",
      "@Msdebramaye I heard about that contest! Congrats girl!!\n",
      "positive\n",
      "\n",
      "\n",
      "UNC!!! NCAA Champs!! Franklin St.: I WAS THERE!! WILD AND CRAZY!!!!!! Nothing like it...EVER http://tinyurl.com/49955t3\n",
      "positive\n",
      "\n",
      "\n",
      "Do you Share More #jokes #quotes #music #photos or #news #articles on #Facebook or #Twitter?\n",
      "positive\n",
      "\n",
      "\n",
      "Good night #Twitter and #TheLegionoftheFallen.  5:45am cimes awfully early!\n",
      "positive\n",
      "\n",
      "\n",
      "\"|I just finished a 2.66 mi run with a pace of 11'14\"\"/mi with Nike+ GPS. #nikeplus #makeitcount|\"\n",
      "positive\n",
      "\n",
      "\n",
      "Disappointing day. Attended a car boot sale to raise some funds for the sanctuary, made a total of 88p after the entry fee - sigh\n",
      "negative\n",
      "\n",
      "\n",
      "no more taking Irish car bombs with strange Australian women who can drink like rockstars...my head hurts.\n",
      "positive\n",
      "\n",
      "\n",
      "Just had some bloodwork done. My arm hurts\n",
      "positive\n",
      "\n",
      "\n",
      "\"|the rock is destined to be the 21st century's new \"\" conan \"\" and that he's going to make a splash even greater than arnold schwarzenegger \n",
      "positive\n",
      "\n",
      "\n",
      "\"|the gorgeously elaborate continuation of \"\" the lord of the rings \"\" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .|\"\n",
      "positive\n",
      "\n",
      "\n",
      "effective but too-tepid biopic\n",
      "positive\n",
      "\n",
      "\n",
      "\"|if you sometimes like to go to the movies to have fun \n",
      "positive\n",
      "\n",
      "\n",
      "\"|emerges as something rare \n",
      "positive\n",
      "\n",
      "\n",
      "the film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\n",
      "positive\n",
      "\n",
      "\n",
      "offers that rare combination of entertainment and education .\n",
      "positive\n",
      "\n",
      "\n",
      "perhaps no picture ever made has more literally showed that the road to hell is paved with good intentions .\n",
      "positive\n",
      "\n",
      "\n",
      "steers turns in a snappy screenplay that curls at the edges ; it's so clever you want to hate it . but he somehow pulls it off .\n",
      "negative\n",
      "\n",
      "\n",
      "take care of my cat offers a refreshingly different slice of asian cinema .\n",
      "positive\n",
      "\n",
      "\n",
      "a mimetic approximation of better films like contempt and 8 1/2 .\n",
      "positive\n",
      "\n",
      "\n",
      "\"|unintelligible \n",
      "positive\n",
      "\n",
      "\n",
      "\"|watching the powerpuff girls movie \n",
      "negative\n",
      "\n",
      "\n",
      "aan opportunity wasted .\n",
      "positive\n",
      "\n",
      "\n",
      "an inelegant combination of two unrelated shorts that falls far short of the director's previous work in terms of both thematic content and narrative strength .\n",
      "positive\n",
      "\n",
      "\n",
      "\"|to build a feel-good fantasy around a vain dictator-madman is off-putting \n",
      "positive\n",
      "\n",
      "\n",
      "\"|with the cheesiest monsters this side of a horror spoof \n",
      "positive\n",
      "\n",
      "\n",
      "\"|mild \n",
      "positive\n",
      "\n",
      "\n",
      "though its atmosphere is intriguing . . . the drama is finally too predictable to leave much of an impression .\n",
      "negative\n",
      "\n",
      "\n",
      "\"|though this rude and crude film does deliver a few gut-busting laughs \n",
      "positive\n",
      "\n",
      "\n",
      "\"|although it tries to be much more \n",
      "positive\n",
      "\n",
      "\n",
      "\"|Amazes me\n",
      "positive\n",
      "\n",
      "\n",
      "\"|Haven't even freshened up\n",
      "positive\n",
      "\n",
      "\n",
      "\"|The Ice Cream Sandwich presentation was very buggy\n",
      "positive\n",
      "\n",
      "\n",
      "Recent study of #Google CTR reveals that top three positions in #GoogleSearch receive 35% of traffic.... http://t.co/IHts3PyW\n",
      "positive\n",
      "\n",
      "\n",
      "\"|Hey\n",
      "positive\n",
      "\n",
      "\n",
      "\"|#google #yahoo #bing Ballmer on Bing\n",
      "positive\n",
      "\n",
      "\n",
      "So is the #galaxynexus coming to @sprint any info @android #google #android\n",
      "positive\n",
      "\n",
      "\n",
      "#Google Google Event: Vorstellung des Samsung Galaxy Nexus (Caschys Blog): o3:30 klingelte mein Wecker. Googles ... http://t.co/QnVDVcMX\n",
      "positive\n",
      "\n",
      "\n",
      "That was good but when is it coming out? What carriers? What pricepoints? #android #google\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import pprint\n",
    "import nltk.classify\n",
    "from itertools import groupby\n",
    "\n",
    "#--------Preprocessing phase (remove redundant words such a Hashtag,sms lingo,url......)\n",
    "def HashTagSplit(text):\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "    probs, lasts = [1.0], [0]\n",
    "    for i in range(1, len(text) + 1):\n",
    "        prob_k, k = max((probs[j] * word_prob(text[j:i]), j)\n",
    "                        for j in range(max(0, i - max_word_length), i))\n",
    "        probs.append(prob_k)\n",
    "        lasts.append(k)\n",
    "        \n",
    "    words = []\n",
    "    i = len(text)\n",
    "    j = 0\n",
    "        \n",
    "    while 0 < i:\n",
    "        words.append(text[lasts[i]:i])\n",
    "        j = j+1\n",
    "        i = lasts[i]\n",
    "    words.reverse()\n",
    "    #print(words)\n",
    "    finalword = \"\"\n",
    "            \n",
    "    for i in range(0,j):\n",
    "        finalword = finalword +'\\t' +words[i]\n",
    "    return finalword #probs[-1]\n",
    "        \n",
    "        \n",
    "def word_prob(word): return dictionary.get(word, 0) / total\n",
    "def words(text): return re.findall('[a-z]+', text.lower())\n",
    "\n",
    "dictionary = dict((w, len(list(ws)))\n",
    "                  for w, ws in groupby(sorted(words(open('Desktop/naive_bayes/data/words.txt').read()))))\n",
    "max_word_length = max(map(len, dictionary))\n",
    "total = float(sum(dictionary.values()))\n",
    "\n",
    "#HashTagSplit(\"elnaz#baktash\")\n",
    "\n",
    "#--------Remove Sms Lingo words--------------------------\n",
    "def smslingo(message):   \n",
    "    alphaindex = {}\n",
    "    count = []\n",
    "    vocab = {}\n",
    "    abb = []\n",
    "    dictionary = [[0 for x in range(200)] for x in range(800)]\n",
    "    f1 = open('Desktop/naive_bayes/data/dictionary1.txt',\"r+\")\n",
    "    f2 = open('Desktop/naive_bayes/data/dictionary2.txt',\"r+\")\n",
    "    f3 = open('Desktop/naive_bayes/data/count.txt',\"r+\")\n",
    "    f5 = open('Desktop/naive_bayes/data/label.txt',\"r+\")\n",
    "    \n",
    "    tmp = 0\n",
    "    put = 0\n",
    "    for words in f1.readlines():\n",
    "        vocab[tmp]=words.lower()\n",
    "        tmp = tmp +1\n",
    "    \n",
    "    tmp=0\n",
    "    \n",
    "    for words in f2.readlines():\n",
    "        words=words.lower()\n",
    "        abb.append(words)\n",
    "        tmp = tmp +1\n",
    "        \n",
    "    counter= 0\n",
    "    for words in f3.readlines():\n",
    "        count.append(words)\n",
    "        counter = counter+1\n",
    "        \n",
    "    alpha =0\n",
    "    for words in f5.readlines():\n",
    "        alphaindex[words]=alpha\n",
    "        alpha = alpha + 1\n",
    "\n",
    "        \n",
    "    track =0\n",
    "    put =0\n",
    "    get =0\n",
    "    \n",
    "    #------create a dictionary from dictionary1 & dictionary2----------------------\n",
    "    for i in range(0,counter,27):\n",
    "        for j in range(1,28):\n",
    "            temporary = count[track]\n",
    "            temporary = int(temporary)\n",
    "            for i in range(0,temporary):\n",
    "                first = vocab[get].lower().rstrip('\\n')\n",
    "                second = abb[get].lower().rstrip('\\n')\n",
    "                get = get + 1\n",
    "                dictionary[put][i] = {first:second}\n",
    "                #print(dictionary[put][i])\n",
    "            put = put+1\n",
    "            track  = track  +1\n",
    "    \n",
    "   \n",
    "    #--------------messages = smstext.split()----------------------\n",
    "    keep = 0\n",
    "    words = message\n",
    "    #print(words)\n",
    "    \n",
    "    words = words.lower()\n",
    "    first = words[0]\n",
    "    #print(first)\n",
    "    \n",
    "    if(len(words)>=2):\n",
    "        second = words[1]        \n",
    "        letter = first + second\n",
    "        #print(letter)\n",
    "        \n",
    "    else:\n",
    "        letter = first\n",
    "        \n",
    "    for k,v in alphaindex.items():\n",
    "        if set (letter.split()) & set(k.split()):\n",
    "            index = v\n",
    "            keep = 1\n",
    "    if keep==0:\n",
    "        letter = first\n",
    "        for k,v in alphaindex.items():\n",
    "            if set (letter.split()) & set(k.split()):\n",
    "                index = v\n",
    "    #print(alphaindex.items())\n",
    "        \n",
    "    loop  = count[index]\n",
    "    #print(loop)\n",
    "    lt = int(loop)\n",
    "    #print(lt)\n",
    "    for i in range(0,lt):\n",
    "        for k,v in dictionary[index][i].items():\n",
    "            if set (words.split()) & set(k.split()):\n",
    "                keep = 2\n",
    "                replace = v\n",
    "                #print(v)                \n",
    "    if keep==2:\n",
    "        replaceword = replace\n",
    "        #print(replaceword)\n",
    "    else:\n",
    "        replaceword = words\n",
    "    return replaceword\n",
    "            \n",
    "#smslingo(\"u use it\")\n",
    "\n",
    "#----------------start replaceTwoOrMore(look for 2 or more repetitions of character)------------------------------\n",
    "def replaceTwoOrMore(s):\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    #print(pattern)\n",
    "    return pattern.sub(r\"\\1\\1\", s)\n",
    "#replaceTwoOrMore(\"ses\")\n",
    "\n",
    "#---------start process_tweet---------------------------------------\n",
    "def processTweet(tweet):\n",
    "    #----Convert to lower case-----\n",
    "    tweet = tweet.lower()\n",
    "    #-------Convert www.* or https?://* to URL-----------\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #-------Convert @username to AT_USER--------------\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    #-----------Remove additional white spaces----------\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #----------Replace hash tag words after spliting them---------\n",
    "    tweet = re.sub(r'#([^\\s]+)', lambda x: HashTagSplit(x.group()), tweet)\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    return tweet\n",
    "#------------start getStopWordList(Common words)---------------------\n",
    "def getStopWordList(stopWordListFileName):\n",
    "    #-----read the stopwords-------\n",
    "    stopWords = []\n",
    "    stopWords.append('AT_USER')\n",
    "    stopWords.append('URL')\n",
    "    #print(stopWords)\n",
    "    fp = open(stopWordListFileName, 'r')\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        word = line.strip()\n",
    "        stopWords.append(word)\n",
    "        line = fp.readline()\n",
    "    fp.close()\n",
    "    return stopWords\n",
    "#getStopWordList('Desktop/naive_bayes/data/stopwords.txt')\n",
    "\n",
    "#-----------#start getfeatureVector----------------------\n",
    "def getFeatureVector(tweet, stopWords):\n",
    "    featureVector = []\n",
    "    words = tweet.split()\n",
    "    for w in words:\n",
    "        #-----replace two or more with two occurrences-------\n",
    "        w = replaceTwoOrMore(w)\n",
    "        #------strip punctuation---------\n",
    "        w = w.strip('\\'\"?,.')\n",
    "        #---------check if it consists of only words------------\n",
    "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*[a-zA-Z]+[a-zA-Z0-9]*$\", w)\n",
    "        #---------ignore if it is a stopWord----------------\n",
    "        if(w in stopWords or val is None):\n",
    "            continue\n",
    "        else:\n",
    "            w = smslingo(w)\n",
    "            featureVector.append(w.lower())\n",
    "    return featureVector\n",
    "\n",
    "    #-----------start extract_features-----------------\n",
    "def extract_features(tweet):\n",
    "    #print(tweet)\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in tweet_words:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features\n",
    "#--------NaiveBayes Classifier-----------------------------\n",
    "def naivebayesclassifier(train,test):\n",
    "    inpTweets = csv.reader(open(train,'rt'), delimiter=',', quotechar='|')\n",
    "    stopWords = getStopWordList('Desktop/naive_bayes/data/stopwords.txt')\n",
    "    count = 0;\n",
    "    featureList = []\n",
    "    tweets = []\n",
    "    \n",
    "    inputtweet = {}\n",
    "    outputtweet = {}\n",
    "    \n",
    "    \n",
    "    for row in inpTweets:\n",
    "        sentiment = row[0]\n",
    "        tweet = row[1]\n",
    "        labels.append(sentiment)\n",
    "        #print (tweet)\n",
    "        #print ('\\n')\n",
    "        processedTweet = processTweet(tweet)\n",
    "        featureVector = getFeatureVector(processedTweet, stopWords)\n",
    "        featureList.extend(featureVector)\n",
    "        #print(featureList)\n",
    "        tweets.append((featureVector, sentiment));\n",
    "        #print(tweets)\n",
    "    #-----------Remove featureList duplicates-----------\n",
    "    featureList = list(set(featureList))\n",
    "    f6 = open('featurelist.txt','w+')\n",
    "    #-----------Create Features vector list------------\n",
    "    for features in featureList:\n",
    "        newfeatures = features + '\\t'\n",
    "        f6.write(newfeatures)\n",
    "    #------------Generate the training set--------------\n",
    "    training_set = nltk.classify.util.apply_features(extract_features, tweets)\n",
    "    #print(training_set)\n",
    "    #------------Train the Naive Bayes classifier-------------------\n",
    "    NBClassifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "    \n",
    "    #-----------Test the classifier-------------------------------\n",
    "    inpTweets = csv.reader(open(test,'rt'), delimiter=',', quotechar='|')\n",
    "  \n",
    "    for row in inpTweets:\n",
    "        line = row[0]\n",
    "        print ('\\n')\n",
    "        print (line)\n",
    "        processedTestTweet = processTweet(line)\n",
    "        sentiment = NBClassifier.classify(extract_features(getFeatureVector(processedTestTweet, stopWords)))\n",
    "        line = line.rstrip('\\n')    \n",
    "        outputtweet[line] = sentiment\n",
    "        print(outputtweet[line])\n",
    "#-------------Main-------------------\n",
    "labels = []\n",
    "\n",
    "naivebayesclassifier(\"Desktop/naive_bayes/data/train.csv\",\"Desktop/naive_bayes/data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
